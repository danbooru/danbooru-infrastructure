# https://postgresqlco.nf/tuning-guide
# https://github.com/jberkus/annotated.conf
# https://people.freebsd.org/~seanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf
# https://gist.github.com/saurabhnanda/5258207935bf23cd112be292d22f00d5
# https://pg.uptrace.dev/zfs/

# Specifies the TCP/IP address(es) on which the server is to listen for
# connections from client applications. The value takes the form of a
# comma-separated list of host names and/or numeric IP addresses. The special
# entry * corresponds to all available IP interfaces.
listen_addresses = '*'

# The TCP port the server listens on; 5432 by default.
port = 5432

# Determines the maximum number of concurrent connections to the database server.
max_connections = 500

# Specifies the amount of time with no network activity after which the
# operating system should send a TCP keepalive message to the client.
tcp_keepalives_idle = 180

# Specifies the amount of time after which a TCP keepalive message that has not
# been acknowledged by the client should be retransmitted.
tcp_keepalives_interval = 15

# Specifies the number of TCP keepalive messages that can be lost before the
# server's connection to the client is considered dead.
tcp_keepalives_count = 4

# Sets the amount of memory the database server uses for shared memory buffers.
# If you have a dedicated database server with 1GB or more of RAM, a reasonable
# starting value for shared_buffers is 25% of the memory in your system. Larger
# settings for shared_buffers usually require a corresponding increase in
# max_wal_size, in order to spread out the process of writing large quantities
# of new or changed data over a longer period of time.
shared_buffers = 96GB

# Sets the planner's assumption about the effective size of the disk cache that
# is available to a single query. This is factored into estimates of the cost
# of using an index; a higher value makes it more likely index scans will be
# used, a lower value makes it more likely sequential scans will be used. When
# setting this parameter you should consider both PostgreSQL's shared buffers
# and the portion of the kernel's disk cache that will be used for PostgreSQL
# data files, though some data might exist in both places. Also, take into
# account the expected number of concurrent queries on different tables, since
# they will have to share the available space. This parameter has no effect on
# the size of shared memory allocated by PostgreSQL, nor does it reserve kernel
# disk cache; it is used only for estimation purposes. The system also does not
# assume data remains in the disk cache between queries.
effective_cache_size = 96GB

# Sets the base maximum amount of memory to be used by a query operation (such
# as a sort or hash table) before writing to temporary disk files. If this
# value is specified without units, it is taken as kilobytes. The default value
# is four megabytes (4MB). Note that for a complex query, several sort or hash
# operations might be running in parallel; each operation will generally be
# allowed to use as much memory as this value specifies before it starts to
# write data into temporary files. Also, several running sessions could be
# doing such operations concurrently. Therefore, the total memory used could be
# many times the value of work_mem; it is necessary to keep this fact in mind
# when choosing the value. Sort operations are used for ORDER BY, DISTINCT, and
# merge joins. Hash tables are used in hash joins, hash-based aggregation,
# result cache nodes and hash-based processing of IN subqueries.
work_mem = 48MB

# Specifies the maximum amount of memory to be used by maintenance operations,
# such as VACUUM, CREATE INDEX, and ALTER TABLE ADD FOREIGN KEY. If this value
# is specified without units, it is taken as kilobytes. It defaults to 64
# megabytes (64MB). Since only one of these operations can be executed at a
# time by a database session, and an installation normally doesn't have many of
# them running concurrently, it's safe to set this value significantly larger
# than work_mem. Larger settings might improve performance for vacuuming and
# for restoring database dumps.
maintenance_work_mem = 2GB

# Sets the number of concurrent disk I/O operations that PostgreSQL expects can
# be executed simultaneously. Raising this value will increase the number of
# I/O operations that any individual PostgreSQL session attempts to initiate in
# parallel. The allowed range is 1 to 1000, or zero to disable issuance of
# asynchronous I/O requests. Currently, this setting only affects bitmap heap
# scans.
effective_io_concurrency = 1000

# Sets the maximum number of background processes that the system can support.
# When running a standby server, you must set this parameter to the same or
# higher value than on the primary server. Otherwise, queries will not be
# allowed in the standby server.
max_worker_processes = 64

# Sets the maximum number of workers that the system can support for parallel operations.
max_parallel_workers = 60

# Sets the maximum number of workers that can be started by a single Gather or
# Gather Merge node. Parallel workers are taken from the pool of processes
# established by max_worker_processes, limited by max_parallel_workers. Note
# that the requested number of workers may not actually be available at run
# time. If this occurs, the plan will run with fewer workers than expected,
# which may be inefficient.

# Note that parallel queries may consume very substantially more resources than
# non-parallel queries, because each worker process is a completely separate
# process which has roughly the same impact on the system as an additional user
# session. This should be taken into account when choosing a value for this
# setting, as well as when configuring other settings that control resource
# utilization, such as work_mem. Resource limits such as work_mem are applied
# individually to each worker, which means the total utilization may be much
# higher across all processes than it would normally be for any single process.
# For example, a parallel query using 4 workers may use up to 5 times as much
# CPU time, memory, I/O bandwidth, and so forth as a query which uses no
# workers at all.
max_parallel_workers_per_gather = 4

# Sets the planner's estimate of the cost of a disk page fetch that is part of
# a series of sequential fetches.
seq_page_cost = 1.0

# Sets the planner's estimate of the cost of a non-sequentially-fetched disk
# page. Reducing this value relative to seq_page_cost will cause the system to
# prefer index scans; raising it will make index scans look relatively more
# expensive. You can raise or lower both values together to change the
# importance of disk I/O costs relative to CPU costs.
random_page_cost = 1.0

# wal_level determines how much information is written to the WAL. The default
# value is replica, which writes enough data to support WAL archiving and
# replication, including running read-only queries on a standby server. minimal
# removes all logging except the information required to recover from a crash
# or immediate shutdown. Finally, logical adds information necessary to support
# logical decoding. Each level includes the information logged at all lower
# levels. This parameter can only be set at server start.
wal_level = replica

# When this parameter is on, the PostgreSQL server writes the entire content of
# each disk page to WAL during the first modification of that page after a
# checkpoint. This is needed because a page write that is in process during an
# operating system crash might be only partially completed, leading to an
# on-disk page that contains a mix of old and new data. The row-level change
# data normally stored in WAL will not be enough to completely restore such a
# page during post-crash recovery. Storing the full page image guarantees that
# the page can be correctly restored, but at the price of increasing the amount
# of data that must be written to WAL. (Because WAL replay always starts from a
# checkpoint, it is sufficient to do this during the first change of each page
# after a checkpoint. Therefore, one way to reduce the cost of full-page writes
# is to increase the checkpoint interval parameters.)

# Turning this parameter off speeds normal operation, but might lead to either
# unrecoverable data corruption, or silent data corruption, after a system
# failure. The risks are similar to turning off fsync, though smaller, and it
# should be turned off only based on the same circumstances recommended for
# that parameter.
full_page_writes = off

# If set to on (the default), this option causes WAL files to be recycled by
# renaming them, avoiding the need to create new ones. On COW file systems, it
# may be faster to create new ones, so the option is given to disable this
# behavior.
wal_recycle = off

# Specifies the minimum size of past log file segments kept in the pg_wal
# directory, in case a standby server needs to fetch them for streaming
# replication. If a standby server connected to the sending server falls behind
# by more than wal_keep_size megabytes, the sending server might remove a WAL
# segment still needed by the standby, in which case the replication connection
# will be terminated. Downstream connections will also eventually fail as a
# result. (However, the standby server can recover by fetching the segment from
# archive, if WAL archiving is in use.)

# This sets only the minimum size of segments retained in pg_wal; the system
# might need to retain more segments for WAL archival or to recover from a
# checkpoint. If wal_keep_size is zero (the default), the system doesn't keep
# any extra segments for standby purposes, so the number of old WAL segments
# available to standby servers is a function of the location of the previous
# checkpoint and status of WAL archiving. If this value is specified without
# units, it is taken as megabytes. This parameter can only be set in the
# postgresql.conf file or on the server command line.
wal_keep_size = 4GB

# https://www.postgresql.org/docs/current/jit.html
jit = off

# https://www.postgresql.org/docs/current/runtime-config-logging.html
log_destination = stderr
log_line_prefix = '[%c] %qclient="%r" db="%u@%d" app="%a": '

# ddl logs all data definition statements, such as CREATE, ALTER, and DROP statements.
log_statement = ddl

# Causes each attempted connection to the server to be logged, as well as
# successful completion of both client authentication (if necessary) and
# authorization.
log_connections = on

# Causes session terminations to be logged. The log output provides information
# similar to log_connections, plus the duration of the session.
log_disconnections = on

# Causes checkpoints and restartpoints to be logged in the server log. Some
# statistics are included in the log messages, including the number of buffers
# written and the time spent writing them.
log_checkpoints = on

# Enables timing of database I/O calls. This parameter is off by default, as it
# will repeatedly query the operating system for the current time, which may
# cause significant overhead on some platforms. You can use the pg_test_timing
# tool to measure the overhead of timing on your system. I/O timing information
# is displayed in pg_stat_database, in the output of EXPLAIN when the BUFFERS
# option is used, by autovacuum for auto-vacuums and auto-analyzes, when
# log_autovacuum_min_duration is set and by pg_stat_statements.
track_io_timing = on

# Enables timing of WAL I/O calls. This parameter is off by default, as it will
# repeatedly query the operating system for the current time, which may cause
# significant overhead on some platforms. You can use the pg_test_timing tool
# to measure the overhead of timing on your system. I/O timing information is
# displayed in pg_stat_wal.
track_wal_io_timing = on

# Sets the maximum size of a GIN index's pending list, which is used when
# fastupdate is enabled. If the list grows larger than this maximum size, it is
# cleaned up by moving the entries in it to the index's main GIN data structure
# in bulk.
gin_pending_list_limit = 4MB

timezone = UTC
